{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_11_Q1(Text Mining)(.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **1) Perform sentimental analysis on the Elon-musk tweets (Exlon-musk.csv)**"
      ],
      "metadata": {
        "id": "b9PalnNeGi_n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqXTrSzYGKCn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import spacy\n",
        "\n",
        "from matplotlib.pyplot import imread\n",
        "from wordcloud import WordCloud, STOPWORDS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets=pd.read_csv('Elon_musk.csv',encoding='Latin-1')\n",
        "tweets.drop(['Unnamed: 0'],inplace=True,axis=1)\n",
        "tweets"
      ],
      "metadata": {
        "id": "qMKRsDjxGwt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Text Preprocessing**"
      ],
      "metadata": {
        "id": "gyAi5lCiHGqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets=[Text.strip() for Text in tweets.Text] # remove both the leading and the trailing characters\n",
        "tweets=[Text for Text in tweets if Text] # removes empty strings, because they are considered in Python as False\n",
        "tweets[0:10]"
      ],
      "metadata": {
        "id": "cHZs8F5xG21A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_text=' '.join(tweets)\n",
        "tweets_text"
      ],
      "metadata": {
        "id": "inFKleVzHQcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "tknzr = TweetTokenizer(strip_handles=True)\n",
        "tweets_tokens=tknzr.tokenize(tweets_text)\n",
        "print(tweets_tokens)"
      ],
      "metadata": {
        "id": "oZMO2H4HHVCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_tokens_text=' '.join(tweets_tokens)\n",
        "tweets_tokens_text"
      ],
      "metadata": {
        "id": "0-buUB_XHlDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "no_punc_text=tweets_tokens_text.translate(str.maketrans('','',string.punctuation))\n",
        "no_punc_text"
      ],
      "metadata": {
        "id": "wuuaavv_HtSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "no_url_text=re.sub(r'http\\S+', '', no_punc_text)\n",
        "no_url_text"
      ],
      "metadata": {
        "id": "GdTQoMA5H0Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "9xPRg2R1H5YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "text_tokens=word_tokenize(no_url_text)\n",
        "print(text_tokens)"
      ],
      "metadata": {
        "id": "ys1GYs4fH_Uv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "KhiVOAFqICVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_tokens)"
      ],
      "metadata": {
        "id": "dlgoYYfZIIQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "my_stop_words=stopwords.words('english')\n",
        "\n",
        "sw_list = ['\\x92','rt','ye','yeah','haha','Yes','U0001F923','I']\n",
        "my_stop_words.extend(sw_list)\n",
        "\n",
        "no_stop_tokens=[word for word in text_tokens if not word in my_stop_words]\n",
        "print(no_stop_tokens)"
      ],
      "metadata": {
        "id": "Q8QA3S-ZINXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lower_words=[Text.lower() for Text in no_stop_tokens]\n",
        "print(lower_words[100:200])"
      ],
      "metadata": {
        "id": "j_616o2qIP_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps=PorterStemmer()\n",
        "stemmed_tokens=[ps.stem(word) for word in lower_words]\n",
        "print(stemmed_tokens[100:200])"
      ],
      "metadata": {
        "id": "9dW6L4LNIVtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install en_core_web_sm\n",
        "!python -m spacy download en"
      ],
      "metadata": {
        "id": "AKsQpPObIYsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en.examples import sentences\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "doc=nlp(' '.join(lower_words))\n",
        "print(doc)"
      ],
      "metadata": {
        "id": "OGztFkzRIbpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmas=[token.lemma_ for token in doc]\n",
        "print(lemmas)"
      ],
      "metadata": {
        "id": "-EatgDlMIk0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_tweets=' '.join(lemmas)\n",
        "clean_tweets"
      ],
      "metadata": {
        "id": "A3s-FJLtIqrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Extaction**"
      ],
      "metadata": {
        "id": "MFun50h5I1dR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Using CountVectorizer**"
      ],
      "metadata": {
        "id": "Chp07KgPJFwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv=CountVectorizer()\n",
        "tweetscv=cv.fit_transform(lemmas)"
      ],
      "metadata": {
        "id": "Df4Xipx1IxWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cv.vocabulary_)"
      ],
      "metadata": {
        "id": "_EdZhuItJK55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cv.get_feature_names()[100:200])"
      ],
      "metadata": {
        "id": "DWQSQVgGJO2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tweetscv.toarray()[100:200])"
      ],
      "metadata": {
        "id": "RjftrqhtJSPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tweetscv.toarray().shape)"
      ],
      "metadata": {
        "id": "SDf61f3IJXRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. CountVectorizer with N-grams (Bigrams & Trigrams)**"
      ],
      "metadata": {
        "id": "k71b3PacJgiw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cv_ngram_range=CountVectorizer(analyzer='word',ngram_range=(1,3),max_features=100)\n",
        "bow_matrix_ngram=cv_ngram_range.fit_transform(lemmas)"
      ],
      "metadata": {
        "id": "x_9_MMWKJbae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(cv_ngram_range.get_feature_names())\n",
        "print(bow_matrix_ngram.toarray())"
      ],
      "metadata": {
        "id": "TQakFF89JpGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. TF-IDF Vectorizer**"
      ],
      "metadata": {
        "id": "DQSP2w6HJ0df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidfv_ngram_max_features=TfidfVectorizer(norm='l2',analyzer='word',ngram_range=(1,3),max_features=500)\n",
        "tfidf_matix_ngram=tfidfv_ngram_max_features.fit_transform(lemmas)"
      ],
      "metadata": {
        "id": "TngBOQkAJt3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tfidfv_ngram_max_features.get_feature_names())\n",
        "print(tfidf_matix_ngram.toarray())"
      ],
      "metadata": {
        "id": "yrdvffk9J72-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Generate Word Cloud**"
      ],
      "metadata": {
        "id": "3ANWkqHWKDnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_cloud(wordcloud):\n",
        "    plt.figure(figsize=(40,30))\n",
        "    plt.imshow(wordcloud)\n",
        "    plt.axis('off')\n",
        "    STOPWORDS.add('pron')\n",
        "STOPWORDS.add('rt')\n",
        "STOPWORDS.add('yeah')\n",
        "wordcloud=WordCloud(width=3000,height=2000,background_color='black',max_words=50,\n",
        "                   colormap='Set1',stopwords=STOPWORDS).generate(clean_tweets)\n",
        "plot_cloud(wordcloud)"
      ],
      "metadata": {
        "id": "z7nJviuGJ_EU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Named Entity Recognition (NER)**"
      ],
      "metadata": {
        "id": "a8-ChRIyKadj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp=spacy.load('en_core_web_sm')\n",
        "\n",
        "one_block=clean_tweets\n",
        "doc_block=nlp(one_block)\n",
        "spacy.displacy.render(doc_block,style='ent',jupyter=True)"
      ],
      "metadata": {
        "id": "fJcW6DEIKQsS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc_block[100:200]:\n",
        "    print(token,token.pos_)\n"
      ],
      "metadata": {
        "id": "PZDceo9UKfmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nouns_verbs=[token.text for token in doc_block if token.pos_ in ('NOUN','VERB')]\n",
        "print(nouns_verbs[100:200])"
      ],
      "metadata": {
        "id": "fVP-OmwXKoWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv=CountVectorizer()\n",
        "\n",
        "X=cv.fit_transform(nouns_verbs)\n",
        "sum_words=X.sum(axis=0)\n",
        "\n",
        "words_freq=[(word,sum_words[0,idx]) for word,idx in cv.vocabulary_.items()]\n",
        "words_freq=sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "wd_df=pd.DataFrame(words_freq)\n",
        "wd_df.columns=['word','count']\n",
        "wd_df[0:10] "
      ],
      "metadata": {
        "id": "t9ipovNeK2kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wd_df[0:10].plot.bar(x='word',figsize=(12,8),title='Top 10 nouns and verbs');"
      ],
      "metadata": {
        "id": "c6JF5Q8eK8Sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Emotion Mining - Sentiment Analysis**"
      ],
      "metadata": {
        "id": "K22U8KWJLH_4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import tokenize\n",
        "sentences=tokenize.sent_tokenize(' '.join(tweets))\n",
        "sentences"
      ],
      "metadata": {
        "id": "hi2CDCpkLBqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df=pd.DataFrame(sentences,columns=['sentence'])\n",
        "sent_df"
      ],
      "metadata": {
        "id": "Of34XgEDLNHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "WvRN4x5WNtYe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}